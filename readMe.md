# üîà Classificador de √Åudio - Uma aplica√ß√£o te√≥rica de Sinais e Sistemas Lineares com Ci√™ncia de Dados

## üìñ Descri√ß√£o Geral

Este projeto tem como objetivo criar um sistema de classifica√ß√£o autom√°tica de sons usando grava√ß√µes de √°udio. A base de dados utilizada √© o **ESC-50**, que re√∫ne amostras sonoras de diversas categorias, como sons de animais, alarmes, instrumentos e ambientes.

Para as 11 categorias escolhidas, foi aplicada a t√©cnica de **data augmentation** (aumento de dados), gerando varia√ß√µes artificiais dos √°udios originais e ampliando a quantidade de exemplos dispon√≠veis para o treinamento. Em seguida, s√£o extra√≠das **63 features** (caracter√≠sticas) de cada amostra, como MFCCs, centroides espectrais, contraste, entre outras.

Essas caracter√≠sticas alimentam um modelo de rede neural treinado para reconhecer os padr√µes sonoros. O sistema √© integrado a uma interface gr√°fica intuitiva, que permite gravar √°udio ao vivo e exibir, de forma pr√°tica, a classe sonora prevista com base no modelo treinado.


## ‚öô Requisitos

O projeto foi desenvolvido em **Python 3.12.7** e depende das seguintes bibliotecas:

- `numpy`
- `pandas`
- `librosa`
- `scikit-learn`
- `sounddevice`
- `matplotlib`
- `joblib`
- `streamlit`
- `tensorflow`
- `tqdm`
- `seaborn`
- `PyQt5`

Todas as depend√™ncias podem ser instaladas com o seguinte comando:

```bash
pip install -r requirements.txt
```

N√£o h√° requisitos espec√≠ficos de hardware e pode ser executado em qualquer m√°quina com suporte para Python e as bibliotecas acima.

## üèó Estrutura do c√≥digo

De forma geral o projeto est√° centrado em 5 arquivos .py, sendo 4 de configura√ß√£o (**selecionarclasses.py**, **dataaugmentation.py**, **features.py** e **classificador.py**) e 1 de execu√ß√£o do app (**app.py**). 

###### esc50.csv (.\ESC-50-master\meta\esc50.csv)

Esse arquivo csv cont√©m, dentre outras features, as informa√ß√µes de classe e nome do arquivo de √°udio em quest√£o. √â ele quem torna poss√≠vel a navega√ß√£o pelos arquivos de √°udio. Cada √°udio tem 5 segundos de dura√ß√£o.

###### Arquivo 1 - selecionarclasses.py
Esse arquivo √© o ponto de partida do projeto, nele, est√° o script respons√°vel por filtrar as classes e arquivos de √°udio selecionadas para o processo de treinamento do modelo.

As classes s√£o selecionadas a partir de um vetor de strings contendo o nome de cada uma, e depois, um filtro √© aplicado no arquivo esc50.csv para que apenas os arquivos correspondentes possam ser acessados.

As classes selecionadas s√£o divididas em 3 grupos:<br>
- **Animais: Cachorro, gato e sapo**
- **Sons humanos: choro de beb√™, palmas e tosse**
- **Sons cotidianos: batida na porta, teclado, sirene, alarme e sino.**

Esses ser√£o os sons que o app ser√° capaz de reconhecer.

```bash
# Classes
CLASSES_USADAS = [
    'dog', 'cat', 'frog',
    'crying_baby', 'clapping', 'coughing',
    'door_wood_knock', 'keyboard_typing', 'siren', 'clock_alarm', 'church_bells'
]

df = pd.read_csv(CSV_PATH)

# Filtro
df_filtrado = df[df['category'].isin(CLASSES_USADAS)]
print("Total por classe:")
print(df_filtrado['category'].value_counts())
```

Esse filtro tem como sa√≠da um novo arquivo csv **(.\data\esc50_filtrado.csv)** de mesma estrutura que o anterior. 

Al√©m disso, √© mostrado no terminal a quantidade de arquivos de √°udio por classe, totalizando 40 arquivos por classe, totalizando 440 arquivos de entrada inicial.
<p align="center">
  <img src="imgs\classes.png" alt="Arquivos por classe" width="200">
</p>
 
###### Arquivo 2 - dataaugmentation.py
Foi constatado que, com apenas os 440 arquivos de √°udio originais o treinamento n√£o tinha dados o suficiente para ter uma boa efic√°cia nas classifica√ß√µes.
Para resolver isso, foi necess√°rio aplicar efeitos como: **ru√≠do**, **esticar audio**, **altera√ß√£o de tom**, **varia√ß√£o de volume**, **echo** e **invers√£o** aos arquivos originais para que novas varia√ß√µes de cada um sejam criadas.

As fun√ß√µes que descrevem a aplica√ß√£o desses efeitos podem ser vistas abaixo:
```bash
# Fun√ß√µes de aumento de dados
def aplicar_ruido(y, intensidade=0.002):
    ruido = np.random.randn(len(y))
    return y + intensidade * ruido

def esticar_audio(y, taxa):
    return librosa.effects.time_stretch(y, rate=taxa)

def alterar_tom(y, sr, passos):
    return librosa.effects.pitch_shift(y, sr=sr, n_steps=passos)

def alterar_volume(y, ganho_db):
    return y * (10 ** (ganho_db / 20))

def adicionar_echo(y, sr, atraso=0.1, decaimento=0.3):
    atraso_amostras = int(sr * atraso)
    eco = np.zeros(len(y) + atraso_amostras)
    eco[:len(y)] = y
    eco[atraso_amostras:] += decaimento * y
    eco = eco / np.max(np.abs(eco)) 
    return eco.astype(y.dtype)

def inverter_audio(y):
    return y[::-1]
```
A aplica√ß√£o desses efeitos para cada √°udio √© feita de forma rand√¥mica, com cada √°udio tendo 30% de chance de ter  **ru√≠do**, **esticar audio**, **altera√ß√£o de tom** e **varia√ß√£o de volume** e 20% de chance de ter **echo** e **invers√£o** aplicados.
√â importante que seja feito dessa forma rand√¥mica, pois, a cada execu√ß√£o, os novos √°udios gerados com os efeitos aplicados s√£o diferentes. Isso previne um superajuste nos dados e o enviesamento do modelo.

```bash
        # Ru√≠do (30%)
        if random.random() < 0.3:
            y_ruido = aplicar_ruido(y, intensidade=random.uniform(0.001, 0.003))
            nome_ruido = f"noise_{nome_arquivo}"
            sf.write(os.path.join(dir_audios_aumentados, nome_ruido), y_ruido, sr)
            linhas_novas.append({**linha, 'filename': nome_ruido})

        # Altera√ß√£o de tom (30%)
        if random.random() < 0.3:
            passos = random.choice([-1, 1])
            y_tom = alterar_tom(y, sr, passos=passos)
            y_tom = normalizar_audio(y_tom)
            nome_tom = f"pitch_{nome_arquivo}"
            sf.write(os.path.join(dir_audios_aumentados, nome_tom), y_tom, sr)
            linhas_novas.append({**linha, 'filename': nome_tom})

        # Esticar √°udio (30%)
        if random.random() < 0.3:
            taxa = random.choice([0.95, 1.05])
            y_estico = esticar_audio(y, taxa=taxa)
            y_estico = normalizar_audio(y_estico)
            nome_estico = f"stretch_{nome_arquivo}"
            sf.write(os.path.join(dir_audios_aumentados, nome_estico), y_estico, sr)
            linhas_novas.append({**linha, 'filename': nome_estico})

        # Alterar volume (30%)
        if random.random() < 0.3:
            ganho = random.uniform(-3, 3)
            y_volume = alterar_volume(y, ganho)
            y_volume = normalizar_audio(y_volume)
            nome_volume = f"gain_{nome_arquivo}"
            sf.write(os.path.join(dir_audios_aumentados, nome_volume), y_volume, sr)
            linhas_novas.append({**linha, 'filename': nome_volume})

        # Echo (20%)
        if random.random() < 0.2:
            y_eco = adicionar_echo(y, sr, atraso=0.1, decaimento=0.3)
            nome_eco = f"echo_{nome_arquivo}"
            sf.write(os.path.join(dir_audios_aumentados, nome_eco), y_eco, sr)
            linhas_novas.append({**linha, 'filename': nome_eco})

        # Inverter (20%)
        if random.random() < 0.2:
            y_invertido = inverter_audio(y)
            nome_invertido = f"reverse_{nome_arquivo}"
            sf.write(os.path.join(dir_audios_aumentados, nome_invertido), y_invertido, sr)
            linhas_novas.append({**linha, 'filename': nome_invertido})
```

Ap√≥s esse processo, o n√∫mero de arquivos salta de **400** para **1176**, trazendo mais robusutez √† massa de dados e, consequentemente, mais qualidade ao treinamento do modelo.
Como sa√≠da, esse processo retorna um novo arquivo csv **(.\data\esc50_aumentado.csv)** com os mesmos moldes dos anteriores, por√©m, contendo agora tamb√©m os arquivos gerados com efeitos aplicados.

###### Arquivo 3 - features.py
Esse arquivo √© respons√°vel por fazer a extra√ß√£o das features dos arquivos de √°udio.
A extra√ß√£o de caracter√≠sticas foi realizada transformando os sinais de √°udio em representa√ß√µes num√©ricas que pudessem ser utilizadas pelo classificador. Esse processo foi aplicado tanto √†s amostras originais quanto √†s geradas via data augmentation, totalizando 1176 arquivos processados.

O procedimento consistiu nas seguintes etapas:

1) **Leitura dos arquivos de √°udio:** Cada amostra foi carregada com taxa de amostragem de 44.100 Hz.

```bash
y, sr = librosa.load(caminho_audio, sr=44100)
```

2) **Extra√ß√£o de 13 MFCC's (Mel-Frequency Cepstral Coefficients):**
Representam a forma do espectro sonoro e s√£o amplamente utilizados em reconhecimento de fala e sons ambientais.

```bash
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)
    mfcc_mean = np.mean(mfcc, axis=1)
```

3) **Delta e Delta-Delta (13 + 13 coeficientes):**
Primeira e segunda derivadas dos MFCCs, que capturam a varia√ß√£o e acelera√ß√£o das mudan√ßas temporais.

```bash
    delta = librosa.feature.delta(mfcc)
    delta2 = librosa.feature.delta(mfcc, order=2)
    delta_mean = np.mean(delta, axis=1)
    delta2_mean = np.mean(delta2, axis=1)
```

4) **7 Caracter√≠sticas espectrais:**
- **Spectral Centroid (centro de massa espectral)**
```bash
spec_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))
```

- **Spectral Bandwidth (largura de banda)**
```bash
spec_bw = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))
```

- **Spectral Contrast (contraste entre bandas)**
```bash
contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr), axis=1)
```

- **Spectral Roll-off (frequ√™ncia limite de energia acumulada)**
```bash
    rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))
```

5) **ZCR (Zero Crossing Rate):** Quantidade de vezes que o sinal cruza o zero ‚Äî √∫til para identificar sons ruidosos ou tonais

```bash
zcr = np.mean(librosa.feature.zero_crossing_rate(y))
```

6) **RMS (Root Mean Square Energy):** Energia m√©dia do sinal, relacionada ao volume.

```bash
rms = np.mean(librosa.feature.rms(y=y))
```

7) **Chroma STFT (12 coeficientes):** Intensidade das 12 notas musicais
```bash
chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr), axis=1)
```

8) Por fim, todas as features s√£o concatenadas em um √∫nico vetor, representando todas as caracter√≠sticas extra√≠das do arquivo de √°udio

```bash
features = np.concatenate([
    mfcc_mean,       # 13 coeficientes MFCC
    delta_mean,      # 13 coeficientes Delta (1¬™ derivada dos MFCC)
    delta2_mean,     # 13 coeficientes Delta-Delta (2¬™ derivada dos MFCC)
    [spec_centroid], # 1 valor: centroide espectral
    [spec_bw],       # 1 valor: largura de banda espectral
    contrast,        # 7 valores: contraste espectral
    [rolloff],       # 1 valor: roll-off espectral
    [zcr],           # 1 valor: taxa de cruzamento por zero
    [rms],           # 1 valor: energia RMS
    chroma           # 12 valores: intensidade das notas musicais (chroma)
    # Total: 13 + 13 + 13 + 1 + 1 + 7 + 1 + 1 + 1 + 12 = 63
])
```
**Porque calcular a m√©dia?**

Como a maioria das caracter√≠sticas s√£o inicialmente matrizes com varia√ß√µes ao longo do tempo, √© necess√°rio converter esses dados para um vetor de dimens√£o fixa.
 Isso √© feito por meio do c√°lculo da m√©dia de cada coeficiente ao longo do tempo, gerando uma representa√ß√£o compacta da tend√™ncia geral do √°udio, viabilizando seu uso em classificadores.

Ao fim do processo dois vetores s√£o salvos, representando as features extra√≠das dos arquivos de √°udio:
- **X (./features/X.npy):** Vetores com as 63 caracter√≠sticas extra√≠das

- **y (./features/y.npy):** Classes correspondentes

###### Arquivo 4 - Classificador.py
Esse arquivo cont√©m a constru√ß√£o do modelo classificador.
O processo de classifica√ß√£o foi realizado utilizando uma rede neural densa (fully-connected) constru√≠da com a biblioteca TensorFlow/Keras.

Os vetores citados anteriormente, s√£o lidos como entrada para o treinamento.

Antes de serem submentidos ao treinamento, √© necess√°rio um tratamento nessas features.

Para o vetor **X**, que cont√©m os valores num√©ricos, √© necess√°rio uma normaliza√ß√£o dos dados, garantindo que todas as features tenham m√©dia 0 e desvio padr√£o 1. Isso acelera e estabiliza o processo de aprendizado do modelo. O scaler √© salvo para uso posterior no tratamento do √°udio a ser previsto

```bash
scaler = StandardScaler()
X = scaler.fit_transform(X)
dump(scaler, "./models/scaler.joblib")
```

Para o vetor **y**, que cont√©m as "etiquetas" de cada feature, √© usado LabelEncoder para codificar numericamente essas classes.
Depois, √© usado one-hot enconding para representar vetroialmente essa classe dentro do conjunto.
Isso √© necess√°rio para que a fun√ß√£o de sa√≠da 'Softmax' na rede seja usada.

```bash
le = LabelEncoder()
y_enc = le.fit_transform(y)
y_cat = to_categorical(y_enc)  # one-hot encoding
```

Al√©m disso, os dados s√£o dividos em arquivos de treino e teste, usando a fun√ß√£o **train_test_split** do Scikitlearn, ficando:
- 823 para treino **(70%)**
- 353 para teste **(30%)**

**Arquitetura da rede:**
O modelo possui a seguinte estrutura:
```
Camada de entrada com 63 neur√¥nios (uma para cada feature extra√≠da).

Camada densa com 256 unidades e ativa√ß√£o ReLU + Dropout (0.4).

Camada densa com 128 unidades e ativa√ß√£o ReLU + Dropout (0.3).

Camada densa com 64 unidades e ativa√ß√£o ReLU + Dropout (0.2).

Camada de sa√≠da com n√∫mero de neur√¥nios igual ao total de classes, com ativa√ß√£o softmax.
```
Essa arquitetura foi escolhida por ser leve, eficiente e suficientemente expressiva para o volume de dados trabalhado.

O treinamento foi realizado com **100 Epochs** (vezes que os dados s√£o submetidos √† rede) e tem acur√°cia em torno de **89%**.

<p align="center">
  <img src="imgs\treinamento.png" alt="Arquivos por classe" width="400">
</p>

Posteriormente o modelo, o encoder e o hist√≥rico de treinamento s√£o salvos para uso futuro.

```bash
    model.save("./models/audio_nn_model.keras")
    dump(le, "./models/label_encoder.joblib")

    with open('./models/history.pkl', 'wb') as f:
        pickle.dump(history.history, f)
```

###### Arquivo 5 - app.py
Esse arquivo cont√©m a interface gr√°fica interativa para capturar √°udio do microfone, extrair suas features ac√∫sticas e classific√°-lo com base no modelo treinado.

A interface segue um modelo simples, com o seguinte fluxo de intera√ß√£o:
1) O usu√°rio clica no bot√£o.

2) A interface grava o √°udio.

3) A grava√ß√£o termina e √© processada.

4) A classe prevista √© exibida na tela.

A interface tem 3 telas correspondentes e podem ser vistas abaixo:

1) A estrutura do app √© simples, composta apenas por um bot√£o e uma barra que representa o volume, presente no canto inferior direito.

<p align="center">
  <img src="imgs\tela1.png" alt="" width="400">
</p><br>

2) Ao clicar no bot√£o, uma anima√ß√£o √© iniciada, e uma mensagem de "gravando" √© exibida, para mostrar ao usu√°rio que a capta√ß√£o de √°udio est√° ativa por 5 segundos. O n√≠vel do microfone pode ser monitorado pela barra j√° citada.

<p align="center">
  <img src="imgs\tela2.png" alt="" width="400">
</p><br>

3) Ao fim do processamento, a classe √© exibida ao usu√°rio, e o bot√£o fica dispon√≠vel para uma nova classifica√ß√£o, voltando ao estado inicial.

<p align="center">
  <img src="imgs\tela3.png" alt="" width="400">
</p><br>

## üîé Testes

De forma geral, o desempenho do modelo durante os testes se mostrou satisfat√≥rio ao prever corretamente as classes com os quais foi treinado.

√â importante ressaltar que a qualidade da capta√ß√£o tem impacto direto no funcionamento do app e na qualidade dos resultados, portanto, √© recomendado que a capta√ß√£o seja feita usando equipamentos iguais ou semelhantes aos listados abaixos:

**Especifica√ß√µes do equipamento usado para teste:**

| Componente       | Especifica√ß√£o                                       |
|------------------|-----------------------------------------------------|
| **Processador**  | AMD A10-9700 RADEON R7, 10 Compute Cores (4C + 6G), 3.50 GHz |
| **RAM instalada**| 16,0 GB                        |
| **Placa de v√≠deo**| AMD Radeon R7 Graphics (998 MB)                    |
| **Armazenamento**| 224 GB SSD SATA                                     |
| **Microfone**| HyperX QuadCast2                     |